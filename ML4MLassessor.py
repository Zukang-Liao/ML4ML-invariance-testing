# Input: all jason files for all models, and the assessment labels (mlabel.txt and mlabel_t.txt)
# Output:
#      (1) a model database (mdatabase.csv and mdatabase_t.csv), each row of which is all the measurements of one trained model.
#      (2) five trained ML4ML assessors (decision tree, random forest, adaboost, regression tree, linear regression).
#      (3) accuracy on the testing set of the model database for each of the assessor

# Our modeldatabase:
#      partition (a): mid: 1-100, t1-t50. VGG13bn for rotation testing. Test suite: CIFAR10 Testing set.
#      partition (b): mid: 101-200, t101-t150. VGG13bn for brightness testing. Test suite: CIFAR10 Testing set.
#      partition (c): mid: 201-300, t201-t250. VGG13bn for scaling testing. Test suite: CIFAR10 Testing set.
#      partition (d): mid: 301-400, t301-t350. CNN5 for rotation testing. Test suite: MNIST Testing set.
# Mid starting with "t" is a hold-out set. 
# When using three-fold cross validation, the hold-out set is always treated as one fold.
# While the rest "regular" models are randomly split into two folds.


import os
import json
import numpy as np
import pandas as pd
import collections
import argparse
import matplotlib.pyplot as plt
from json_stat import load_json
from sklearn import tree, linear_model
from sklearn.preprocessing import normalize
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.metrics import f1_score, cohen_kappa_score
import shap

# In this work, we only consider "invariance"
target_dict = {1: "rotation", 2: "brightness", 3: "scaling"}

def argparser():
    parser = argparse.ArgumentParser()
    # You must specify:
    # The test_actoverall1515.npy file should be at data_dir/mid/test_actoverall1515.npy
    parser.add_argument("--data_dir", type=str, default="../plots")
    # The test_actoverall1515.npy file for the hold out set should be at data_dir_t/mid/test_actoverall1515.npy
    parser.add_argument("--data_dir_t", type=str, default="")
    # database used to train the CNNs
    parser.add_argument("--dbname", type=str, default="cifar")
    # "r": rotation, "b": brightness, "s": scaling
    parser.add_argument("--aug_type", type=str, default="r")
     # If prepare dataset from stratch
    parser.add_argument("--dataset", type=bool, default=False)


    # Default filename(s)
    # json filename generated by measurement.py default ""
    parser.add_argument("--json_name", type=str, default="")
    # if you have generated test_actoverallxxxx.npy for other testing interval, rather than [-15, 15], please specify the filename here.
    parser.add_argument("--plot_foldername", type=str, default="1515")
    parser.add_argument("--dataset_name", type=str, default="") # filename of "data.csv"
    parser.add_argument("--label_name", type=str, default="mlabel.txt") # filename of "mlabel.txt"
    args = parser.parse_args()


    if args.data_dir_t == "":
        args.data_dir_t = args.data_dir+"_t"
    if args.json_name == "":
        args.json_name = f"stats1515_{args.aug_type}.json"

    if args.dataset_name == "":
        args.dataset_name = f"mdatabase_{args.aug_type}.csv"
        # args.dataset_name = f"mdatabase.csv"
    args.dataset_path = os.path.join(os.path.dirname(args.data_dir), "mdatabase", args.dataset_name)
    args.label_path = os.path.join(os.path.dirname(args.data_dir), "mdatabase", args.label_name)
    args.dataset_path_t = args.dataset_path.rsplit(".", 1)[0]+"_t.csv"
    args.label_path_t = args.label_path.rsplit(".", 1)[0]+"_t.txt"

    print("Datadir: "+args.data_dir)
    print("Datadir_t: "+args.data_dir_t)
    # target_dict = {1: "rotation", 2: "brightness", 3: "scaling"}
    if args.aug_type == "r":
        args.target_idx = 1
    elif args.aug_type == "b":
        args.target_idx = 2
    elif args.aug_type == "s":
        args.target_idx = 3
    return args


def get_midnames(args, test):
    if not test:
        if args.aug_type == "r":
            midnames = [str(i) for i in range(1, 401)]
        else:
            midnames = [str(i) for i in range(1, 301)]
    else:
        midnames = [f"t{i}" for i in range(1, 50+1)]
        midnames += [f"t{i}" for i in range(101, 150+1)] 
        midnames += [f"t{i}" for i in range(201, 250+1)] 
        if args.aug_type == "r":
            midnames += [f"t{i}" for i in range(301, 350+1)]
    return midnames


def prepare_dataset(args, test=False):
    midnames = get_midnames(args, test)
    feature_names = ["mid"]
    intervals = ["1515"]
    intervals_idx = [0]
    sstv_int = ["0.9"]
    sstv_idx = [0]
    chosen_sn = ['max', 'max_mean']
    corrs = ["corr"] # ["all"]
    chosen_classes = ['overall']
    grad_details = True
    # for confidence, conv8 and conv9, 15 properties are measured on 11 intervals.
    # sstv is measured on 9 intervals, but in this work, we use 0.9 only
    if grad_details:
        f_dim = len(chosen_classes) * ((len(sstv_int) + len(intervals) * 15) + (2 * len(chosen_sn) * (len(sstv_int) + len(intervals) * 15))) * len(corrs)
    else:
        f_dim = len(chosen_classes) * ((len(sstv_int) + len(intervals) * 7) + (2 * len(chosen_sn) * (len(sstv_int) + len(intervals) * 7))) * len(corrs)
    # check json_format. 11 classes -- including "overall".
    # *2: corr and incorr 
    f_data = np.zeros([len(midnames), f_dim+1]) # mid for the first colomn
    for i in range(len(midnames)):
        if test:
            jstr = load_json(os.path.join(args.data_dir_t, midnames[i], args.plot_foldername, args.json_name))["test_info"]
        else:
            jstr = load_json(os.path.join(args.data_dir, midnames[i], args.plot_foldername, args.json_name))["test_info"]
        f_data[i, 0] = i + 1
        f_cur = 1
        for c in chosen_classes:
            for fn in jstr[c]:
                if fn == "confidence":
                    for pn in jstr[c][fn]["l2_dist"]:
                        if pn == "grad" and grad_details:
                            for gn in jstr[c][fn]["l2_dist"]["grad"]:
                                for gmn in jstr[c][fn]["l2_dist"]["grad"][gn]:
                                    for corr in corrs:
                                        ff = np.array(jstr[c][fn]["l2_dist"]["grad"][gn][gmn][corr])
                                        lenff = len(intervals)
                                        f_data[i, f_cur:f_cur+lenff] = ff[intervals_idx]
                                        f_cur += lenff
                                        if i == 0:
                                            # feature_names += [f"{c}_{fn}_{pn}_{gn}_{gmn}_{corr}_{j}" for j in intervals]
                                            feature_names += [f"{fn}_{pn}_{gn}_{gmn}" for j in intervals]
                        else:
                            for corr in corrs:
                                if pn == "sstv":
                                    intv = sstv_int
                                    idx = sstv_idx
                                else:
                                    intv = intervals
                                    idx = intervals_idx
                                if i == 0:
                                    # feature_names += [f"{c}_{fn}_{pn}_{corr}_{j}" for j in intv]
                                    feature_names += [f"{fn}_{pn}" for j in intv]
                                ff = np.array(jstr[c][fn]["l2_dist"][pn][corr])
                                lenff = len(intv)
                                f_data[i, f_cur:f_cur+lenff] = ff[idx]
                                f_cur += lenff
                elif "conv" in fn:
                    for sn in chosen_sn:
                        for pn in jstr[c][fn][sn]["l2_dist"]:
                            if pn == "grad" and grad_details:
                                for gn in jstr[c][fn][sn]["l2_dist"]["grad"]:
                                    for gmn in jstr[c][fn][sn]["l2_dist"]["grad"][gn]:
                                        for corr in corrs:
                                            ff = np.array(jstr[c][fn][sn]["l2_dist"]["grad"][gn][gmn][corr])
                                            lenff = len(intervals)
                                            f_data[i, f_cur:f_cur+lenff] = ff[intervals_idx]
                                            f_cur += lenff
                                            if i == 0:
                                                # feature_names += [f"{c}_{fn}_{sn}_{pn}_{gn}_{gmn}_{corr}_{j}" for j in intervals]
                                                feature_names += [f"{fn}_{sn}_{pn}_{gn}_{gmn}" for j in intervals]
                            else:
                                for corr in corrs:
                                    if pn == "sstv":
                                        intv = sstv_int
                                        idx = sstv_idx
                                    else:
                                        intv = intervals
                                        idx = intervals_idx
                                    if i == 0:
                                        # feature_names += [f"{c}_{fn}_{sn}_{pn}_{corr}_{j}" for j in intv]
                                        feature_names += [f"{fn}_{sn}_{pn}" for j in intv]
                                    ff = np.array(jstr[c][fn][sn]["l2_dist"][pn][corr])
                                    lenff = len(intv)
                                    f_data[i, f_cur:f_cur+lenff] = ff[idx]
                                    f_cur += lenff

                else:
                    continue
    dataset_path = args.dataset_path_t if test else args.dataset_path
    df = pd.DataFrame(data=f_data, columns=feature_names)
    df.to_csv(dataset_path, index=False)


def eval(data, labels, clf):
    logits = clf.predict(data)
    pred = [1 if i >= 0.5 else 0 for i in logits]
    return np.sum(pred==labels)/len(labels)


def ml4ml(args, indices, acc_results=[]):
    data = pd.read_csv(args.dataset_path)
    labels = pd.read_csv(args.label_path, sep=" ")
    data_t = pd.read_csv(args.dataset_path_t)
    labels_t = pd.read_csv(args.label_path_t, sep=" ")
    data = data[indices[0]:indices[1]]
    labels = labels[indices[0]:indices[1]]
    data_t = data_t[indices[0]//2:indices[1]//2]
    labels_t = labels_t[indices[0]//2:indices[1]//2]
    nb_trainingset = len(labels)
    indices = np.random.choice(range(nb_trainingset), nb_trainingset, replace=False)
    fold1 = (data.values[indices[:nb_trainingset//2]], labels.values[indices[:nb_trainingset//2]])
    fold2 = (data.values[indices[nb_trainingset//2:]], labels.values[indices[nb_trainingset//2:]])
    fold3 = (data_t.values, labels_t.values)
    target_names = ["invariant", "variance"]
    selected_features = []
    dts = []
    regrs = []
    importances = []

    for fidx in range(3):
        if fidx == 0:
            trainingset = (data.values, labels.values)
            testingset = (data_t.values, labels_t.values)
        elif fidx == 1:
            trainingset = (np.concatenate([fold1[0], fold3[0]]), np.concatenate([fold1[1], fold3[1]]))
            testingset = (fold2[0], fold2[1])
        elif fidx == 2:
            trainingset = (np.concatenate([fold2[0], fold3[0]]), np.concatenate([fold2[1], fold3[1]]))
            testingset = (fold1[0], fold1[1])


        dt = tree.DecisionTreeClassifier()
        dt = dt.fit(trainingset[0][:, 1:], trainingset[1][:, args.target_idx].astype(np.int))
        acc = dt.score(trainingset[0][:, 1:], trainingset[1][:, args.target_idx].astype(np.int))
        acc_t = dt.score(testingset[0][:, 1:], testingset[1][:, args.target_idx].astype(np.int))
        acc_results[fidx]["dt"][target_dict[args.target_idx]].append(acc_t)


        rf = RandomForestClassifier()
        rf = rf.fit(trainingset[0][:, 1:], trainingset[1][:, args.target_idx].astype(np.int))
        acc_rf = rf.score(trainingset[0][:, 1:], trainingset[1][:, args.target_idx].astype(np.int))
        acc_rf_t = rf.score(testingset[0][:, 1:], testingset[1][:, args.target_idx].astype(np.int))
        acc_results[fidx]["rf"][target_dict[args.target_idx]].append(acc_rf_t)


        regr = linear_model.LinearRegression()
        regr = regr.fit(normalize(trainingset[0][:, 1:], axis=1), trainingset[1][:, args.target_idx])
        acc_regr = eval(normalize(trainingset[0][:, 1:], axis=1), trainingset[1][:, args.target_idx], regr)
        acc_regr_t = eval(normalize(testingset[0][:, 1:], axis=1), testingset[1][:, args.target_idx], regr)
        acc_results[fidx]["reg"][target_dict[args.target_idx]].append(acc_regr_t)


        adb = AdaBoostClassifier()
        adb = adb.fit(trainingset[0][:, 1:], trainingset[1][:, args.target_idx].astype(np.int))
        acc_adb = adb.score(trainingset[0][:, 1:], trainingset[1][:, args.target_idx].astype(np.int))
        acc_adb_t = adb.score(testingset[0][:, 1:], testingset[1][:, args.target_idx].astype(np.int))
        acc_results[fidx]["adb"][target_dict[args.target_idx]].append(acc_adb_t)

    return rf



if __name__ == "__main__":
    args = argparser()
    print(args)
    if args.aug_type == "r":
        indices = [0, 100]
        if args.dbname == "mnist":
            indices = [300, 400]
    elif args.aug_type == "b":
        indices = [100, 200]
    elif args.aug_type == "s":
        indices = [200, 300]

    if args.dataset:
        prepare_dataset(args, test=False)
        prepare_dataset(args, test=True)

    acc_results = [{"dt": collections.defaultdict(list), 
                      "reg": collections.defaultdict(list), 
                      "rf": collections.defaultdict(list), 
                      "adb": collections.defaultdict(list)} for i in range(3)]
    for i in range(10):
        rf = ml4ml(args, indices, acc_results=acc_results)
    fold_results = collections.defaultdict(list)
    for clsn in acc_results[0]:
        for task in acc_results[0][clsn]:
            for fold_result in zip(acc_results[0][clsn][task], acc_results[1][clsn][task], acc_results[2][clsn][task]):
                fold_results[clsn].append(np.mean(fold_result))
        acc = np.mean(fold_results[clsn])
        maxx = np.max(fold_results[clsn])
        minn = np.min(fold_results[clsn])
        error = max(np.abs(maxx-acc), np.abs(acc-minn))
        print(f"{task} {clsn} acc: {acc:.3f}, max: {maxx:.3f}, min: {minn:.3f}, err: {error:.3f}")


